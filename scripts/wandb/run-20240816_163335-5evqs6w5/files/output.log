{'seed': 0, 'dataset_name': 'test_1', 'do_train': True, 'load_model': '../../scGPT/save/scGPT_human', 'mask_ratio': 0.0, 'epochs': 10, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 32, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}
save to save/dev_test_1-Aug16-16-33
2024-08-16 16:33:55,105 - Presence thr 10053.85, 7192 genes left
2024-08-16 16:33:55,234 - Presence thr 10053.85, 7192 genes left
2024-08-16 16:33:56,169 - Variance thr 6547654.53, 9085 genes left
2024-08-16 16:33:56,169 - Combined mask nÂº of genes 3000
scGPT - INFO - match 2996/3000 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from ../../scGPT/save/scGPT_human/best_model.pt, the model args will override the config ../../scGPT/save/scGPT_human/args.json.
scGPT - INFO - Normalizing total counts ...
scGPT - INFO - Log1p transforming ...
scGPT - WARNING - The input data seems to be already log1p transformed. Set `log1p=False` to avoid double log1p transform.
scGPT - INFO - Binning data ...
scGPT - INFO - Normalizing total counts ...
scGPT - INFO - Log1p transforming ...
scGPT - WARNING - The input data seems to be already log1p transformed. Set `log1p=False` to avoid double log1p transform.
scGPT - INFO - Binning data ...
scGPT - INFO - train set number of samples: 7619,
	 feature length: 2997
scGPT - INFO - valid set number of samples: 847,
	 feature length: 2997